{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfGmE1i-pZg0",
        "outputId": "3f306a1a-8898-4a81-d602-861dbc3d26d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 python-dotenv-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv huggingface_hub bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6zvbTc0aozZ3"
      },
      "outputs": [],
      "source": [
        "# Configuration and imports\n",
        "import os\n",
        "import yaml\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from huggingface_hub.hf_api import HfFolder\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextGenerationPipeline\n",
        "from utils import email_data_validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ixlNqwEmo9PL"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "HfFolder.save_token(hf_token)\n",
        "\n",
        "# Set the quantization configuration for LLaMA\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\"\n",
        ")\n",
        "\n",
        "# Load the config and the prompts\n",
        "with open(\"config.yaml\", \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "with open(\"prompts.json\", \"r\") as f:\n",
        "    prompts = json.load(f)\n",
        "\n",
        "email_classification_prompt = prompts[\"email_classification\"][config[\"email_prompt\"]]\n",
        "response_generation_prompt = prompts[\"reponse_generation\"][config[\"generation_prompt\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QDP9aXmlo_9G"
      },
      "outputs": [],
      "source": [
        "# Sample email dataset\n",
        "sample_emails = [\n",
        "    {\n",
        "        \"id\": \"001\",\n",
        "        \"from\": \"angry.customer@example.com\",\n",
        "        \"subject\": \"Broken product received\",\n",
        "        \"body\": \"I received my order #12345 yesterday but it arrived completely damaged. This is unacceptable and I demand a refund immediately. This is the worst customer service I've experienced.\",\n",
        "        \"timestamp\": \"2024-03-15T10:30:00Z\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"002\",\n",
        "        \"from\": \"curious.shopper@example.com\",\n",
        "        \"subject\": \"Question about product specifications\",\n",
        "        \"body\": \"Hi, I'm interested in buying your premium package but I couldn't find information about whether it's compatible with Mac OS. Could you please clarify this? Thanks!\",\n",
        "        \"timestamp\": \"2024-03-15T11:45:00Z\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"003\",\n",
        "        \"from\": \"happy.user@example.com\",\n",
        "        \"subject\": \"Amazing customer support\",\n",
        "        \"body\": \"I just wanted to say thank you for the excellent support I received from Sarah on your team. She went above and beyond to help resolve my issue. Keep up the great work!\",\n",
        "        \"timestamp\": \"2024-03-15T13:15:00Z\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"004\",\n",
        "        \"from\": \"tech.user@example.com\",\n",
        "        \"subject\": \"Need help with installation\",\n",
        "        \"body\": \"I've been trying to install the software for the past hour but keep getting error code 5123. I've already tried restarting my computer and clearing the cache. Please help!\",\n",
        "        \"timestamp\": \"2024-03-15T14:20:00Z\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"005\",\n",
        "        \"from\": \"business.client@example.com\",\n",
        "        \"subject\": \"Partnership opportunity\",\n",
        "        \"body\": \"Our company is interested in exploring potential partnership opportunities with your organization. Would it be possible to schedule a call next week to discuss this further?\",\n",
        "        \"timestamp\": \"2024-03-15T15:00:00Z\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM-4xNIKpBHp"
      },
      "outputs": [],
      "source": [
        "class EmailProcessor:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the email processor with LLM API key.\"\"\"\n",
        "\n",
        "        model_name = config[\"llm_model\"]\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
        "        self.pipeline = TextGenerationPipeline(model=self.model, tokenizer=self.tokenizer)\n",
        "\n",
        "        # Define valid categories\n",
        "        self.valid_categories = {\n",
        "            \"complaint\", \"inquiry\", \"feedback\",\n",
        "            \"support_request\", \"other\"\n",
        "        }\n",
        "\n",
        "    def classify_email(self, email: Dict) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Classify an email using LLM.\n",
        "        Returns the classification category or None if classification fails.\n",
        "        \"\"\"\n",
        "\n",
        "        system_prompt = email_classification_prompt\n",
        "        user_prompt = f\"Subject: {email['subject']}, Body: {email['body']}\"\n",
        "\n",
        "        prompt = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_prompt\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            response = self.pipeline(prompt, max_new_tokens=20)[0][\"generated_text\"]\n",
        "            predicted_category = response[-1][\"content\"]\n",
        "\n",
        "            if predicted_category in self.valid_categories:\n",
        "                return predicted_category\n",
        "\n",
        "            return \"other\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Classification failed for email {email['id']}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_response(self, email: Dict, classification: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Generate an automated response based on email classification.\n",
        "        \"\"\"\n",
        "\n",
        "        system_prompt = response_generation_prompt\n",
        "\n",
        "        user_prompt = f\"Subject: {email['subject']}, Body: {email['body']}, Category: {classification}\"\n",
        "\n",
        "        prompt = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_prompt\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            response = self.pipeline(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "            generated_response = response[-1][\"content\"]\n",
        "            print(generated_response)\n",
        "            return generated_response\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Response generation failed for email {email['id']}: {e}\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "camUSZ-SpHTN"
      },
      "outputs": [],
      "source": [
        "class EmailAutomationSystem:\n",
        "    def __init__(self, processor: EmailProcessor):\n",
        "        \"\"\"Initialize the automation system with an EmailProcessor.\"\"\"\n",
        "        self.processor = processor\n",
        "        self.response_handlers = {\n",
        "            \"complaint\": self._handle_complaint,\n",
        "            \"inquiry\": self._handle_inquiry,\n",
        "            \"feedback\": self._handle_feedback,\n",
        "            \"support_request\": self._handle_support_request,\n",
        "            \"other\": self._handle_other\n",
        "        }\n",
        "\n",
        "    def process_email(self, email: Dict) -> Dict:\n",
        "        \"\"\"\n",
        "        Process a single email through the complete pipeline.\n",
        "        Returns a dictionary with the processing results.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            classification = self.processor.classify_email(email)\n",
        "\n",
        "            if not classification:\n",
        "                raise ValueError(f\"Failed to classify email {email['id']}\")\n",
        "\n",
        "            handler = self.response_handlers.get(classification, self._handle_other)\n",
        "\n",
        "            response = self.processor.generate_response(email, classification)\n",
        "            if not response:\n",
        "                raise ValueError(f\"Failed to generate response for email {email['id']}\")\n",
        "\n",
        "            if classification == \"complaint\":\n",
        "                send_complaint_response(email[\"from\"], response)\n",
        "            else:\n",
        "                send_standard_response(email[\"from\"], response)\n",
        "\n",
        "            handler(email)\n",
        "\n",
        "            results = {\n",
        "                \"id\": email[\"id\"],\n",
        "                \"email_id\": email[\"from\"],\n",
        "                \"success\": True,\n",
        "                \"classification\": classification,\n",
        "                \"response_sent\": response\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            results = {\n",
        "                \"id\": email[\"id\"],\n",
        "                \"email_id\": email[\"from\"],\n",
        "                \"success\": False,\n",
        "                \"classification\": None,\n",
        "                \"response_sent\": None\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _handle_complaint(self, email: Dict):\n",
        "        \"\"\"\n",
        "        Handle complaint emails.\n",
        "        \"\"\"\n",
        "        create_urgent_ticket(email[\"from\"], \"complaint\", email[\"body\"])\n",
        "\n",
        "    def _handle_inquiry(self, email: Dict):\n",
        "        \"\"\"\n",
        "        Handle inquiry emails.\n",
        "        \"\"\"\n",
        "        create_support_ticket(email[\"from\"], email[\"body\"])\n",
        "\n",
        "    def _handle_feedback(self, email: Dict):\n",
        "        \"\"\"\n",
        "        Handle feedback emails.\n",
        "        \"\"\"\n",
        "        log_customer_feedback(email[\"from\"], email[\"body\"])\n",
        "\n",
        "    def _handle_support_request(self, email: Dict):\n",
        "        \"\"\"\n",
        "        Handle support request emails.\n",
        "        \"\"\"\n",
        "        create_support_ticket(email[\"from\"], email[\"body\"])\n",
        "\n",
        "    def _handle_other(self, email: Dict):\n",
        "        \"\"\"\n",
        "        Handle other category emails.\n",
        "        \"\"\"\n",
        "        logger.info(f\"Handled email {email['id']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WEcIWViypKqc"
      },
      "outputs": [],
      "source": [
        "# Mock service functions\n",
        "def send_complaint_response(email_id: str, response: str):\n",
        "    \"\"\"Mock function to simulate sending a response to a complaint\"\"\"\n",
        "    logger.info(f\"Sending complaint response for email {email_id}\")\n",
        "    # In real implementation: integrate with email service\n",
        "\n",
        "\n",
        "def send_standard_response(email_id: str, response: str):\n",
        "    \"\"\"Mock function to simulate sending a standard response\"\"\"\n",
        "    logger.info(f\"Sending standard response for email {email_id}\")\n",
        "    # In real implementation: integrate with email service\n",
        "\n",
        "\n",
        "def create_urgent_ticket(email_id: str, category: str, context: str):\n",
        "    \"\"\"Mock function to simulate creating an urgent ticket\"\"\"\n",
        "    logger.info(f\"Creating urgent ticket for email {email_id}\")\n",
        "    # In real implementation: integrate with ticket system\n",
        "\n",
        "\n",
        "def create_support_ticket(email_id: str, context: str):\n",
        "    \"\"\"Mock function to simulate creating a support ticket\"\"\"\n",
        "    logger.info(f\"Creating support ticket for email {email_id}\")\n",
        "    # In real implementation: integrate with ticket system\n",
        "\n",
        "\n",
        "def log_customer_feedback(email_id: str, feedback: str):\n",
        "    \"\"\"Mock function to simulate logging customer feedback\"\"\"\n",
        "    logger.info(f\"Logging feedback for email {email_id}\")\n",
        "    # In real implementation: integrate with feedback system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_o6n9D0dpNhL"
      },
      "outputs": [],
      "source": [
        "def run_demonstration():\n",
        "    \"\"\"Run a demonstration of the complete system.\"\"\"\n",
        "    # Initialize the system\n",
        "    processor = EmailProcessor()\n",
        "    automation_system = EmailAutomationSystem(processor)\n",
        "\n",
        "    # Process all sample emails\n",
        "    results = []\n",
        "    for email in sample_emails:\n",
        "        logger.info(f\"\\nValidating input data for email {email['id']}...\")\n",
        "        if not email_data_validation(email):\n",
        "            raise ValueError(\"Given data has some missing fields!\")\n",
        "        logger.info(f\"\\nProcessing email {email['id']}...\")\n",
        "        result = automation_system.process_email(email)\n",
        "        results.append(result)\n",
        "\n",
        "    # Create a summary DataFrame\n",
        "    df = pd.DataFrame(results)\n",
        "    print(\"\\nProcessing Summary:\")\n",
        "    print(df[[\"id\", \"email_id\", \"success\", \"classification\", \"response_sent\"]])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0b1087c93e5b4b7da4db3ecf1f761e58",
            "11198bc7e1a44f778ffc214c04b7fad0",
            "85d222be4b3a4acb9f70c557c3872792",
            "92941ea211db42af88d9adacaae00384",
            "f85ac842ad754f02b6a93b422dd57097",
            "8a47a4d092324e8daf0e4bb824468540",
            "19e69d77c8e4428a9e875a427454e494",
            "19e61fcbb1934bb5a812435053a97ea2",
            "1d6e12bc38784ccf9f445276cfcf7ff3",
            "6930d83ac3884fd2a999618b9364835d",
            "1918609ca2b44cdfb1323990975909b4"
          ]
        },
        "id": "LnG1-Hf7pPV2",
        "outputId": "c691be18-dd61-43e1-874b-23d57de0610b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b1087c93e5b4b7da4db3ecf1f761e58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subject: Re: Damaged Product Received - Complaint Response\n",
            "\n",
            "Dear [Customer's Name],\n",
            "\n",
            "Thank you for bringing this issue to our attention. We apologize for the unacceptable condition of your order, which is completely unacceptable and not up to our quality standards. We have created an urgent ticket to address this matter and ensure a prompt resolution.\n",
            "\n",
            "Please expect a refund for the damaged product, as well as any additional compensation for the inconvenience caused. Our dedicated team will review the situation and provide a full refund within the next 48 hours.\n",
            "\n",
            "Your satisfaction is our top priority, and we appreciate your feedback. We will use this as an opportunity to review our packaging and shipping procedures to prevent such incidents in the future.\n",
            "\n",
            "Thank you for your patience and understanding.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subject: Re: Question about product specifications\n",
            "\n",
            "Dear [Customer],\n",
            "\n",
            "Thank you for your interest in our premium package. I'm happy to help clarify the compatibility of our product with Mac OS. Our premium package is compatible with Mac OS, and we provide detailed specifications on our website. However, I'd be more than happy to provide you with the specific details you're looking for.\n",
            "\n",
            "Please find the specifications here: [link to product specifications]. If you have any further questions or concerns, feel free to ask.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "\n",
            "A support ticket has been created for your inquiry.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subject: Re: Amazing customer support\n",
            "\n",
            "Dear [Customer],\n",
            "\n",
            "Thank you so much for your kind words about your recent interaction with Sarah on our team! We're thrilled to hear that she was able to provide you with excellent support and resolve your issue to your satisfaction. We're proud of our team for going above and beyond to ensure our customers receive the best possible service.\n",
            "\n",
            "We appreciate your feedback and will continue to strive for excellence in all our interactions.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subject: Re: Support Request - Error Code 5123\n",
            "\n",
            "Dear [Customer Name],\n",
            "\n",
            "Thank you for reaching out to us for assistance. I'm happy to help you resolve the issue with the software installation. I've created a support ticket for you, and I'll be happy to guide you through the troubleshooting process.\n",
            "\n",
            "Can you please try updating the software to the latest version and then try reinstalling it? If the issue persists, we can explore further solutions together.\n",
            "\n",
            "I'll be in touch with you soon to discuss the next steps.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subject: Re: Partnership Opportunity\n",
            "\n",
            "Dear [Customer Name],\n",
            "\n",
            "Thank you for your email expressing interest in a potential partnership opportunity with our company. We're excited to explore possibilities and would be delighted to schedule a call to discuss this further.\n",
            "\n",
            "How about we schedule a call for Wednesday, [Date], at 2 PM EST or Thursday, [Date], at 10 AM EST? Please let us know which time slot works best for you, or suggest an alternative time that suits you.\n",
            "\n",
            "We're looking forward to speaking with you and exploring ways we can collaborate.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "\n",
            "Processing Summary:\n",
            "    id                     email_id  success   classification  \\\n",
            "0  001   angry.customer@example.com     True        complaint   \n",
            "1  002  curious.shopper@example.com     True          inquiry   \n",
            "2  003       happy.user@example.com     True         feedback   \n",
            "3  004        tech.user@example.com     True  support_request   \n",
            "4  005  business.client@example.com     True          inquiry   \n",
            "\n",
            "                                       response_sent  \n",
            "0  Subject: Re: Damaged Product Received - Compla...  \n",
            "1  Subject: Re: Question about product specificat...  \n",
            "2  Subject: Re: Amazing customer support\\n\\nDear ...  \n",
            "3  Subject: Re: Support Request - Error Code 5123...  \n",
            "4  Subject: Re: Partnership Opportunity\\n\\nDear [...  \n"
          ]
        }
      ],
      "source": [
        "results_df = run_demonstration()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IPDxW0LqykC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b1087c93e5b4b7da4db3ecf1f761e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11198bc7e1a44f778ffc214c04b7fad0",
              "IPY_MODEL_85d222be4b3a4acb9f70c557c3872792",
              "IPY_MODEL_92941ea211db42af88d9adacaae00384"
            ],
            "layout": "IPY_MODEL_f85ac842ad754f02b6a93b422dd57097"
          }
        },
        "11198bc7e1a44f778ffc214c04b7fad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a47a4d092324e8daf0e4bb824468540",
            "placeholder": "​",
            "style": "IPY_MODEL_19e69d77c8e4428a9e875a427454e494",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1918609ca2b44cdfb1323990975909b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19e61fcbb1934bb5a812435053a97ea2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e69d77c8e4428a9e875a427454e494": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d6e12bc38784ccf9f445276cfcf7ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6930d83ac3884fd2a999618b9364835d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85d222be4b3a4acb9f70c557c3872792": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19e61fcbb1934bb5a812435053a97ea2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d6e12bc38784ccf9f445276cfcf7ff3",
            "value": 2
          }
        },
        "8a47a4d092324e8daf0e4bb824468540": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92941ea211db42af88d9adacaae00384": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6930d83ac3884fd2a999618b9364835d",
            "placeholder": "​",
            "style": "IPY_MODEL_1918609ca2b44cdfb1323990975909b4",
            "value": " 2/2 [00:37&lt;00:00, 17.17s/it]"
          }
        },
        "f85ac842ad754f02b6a93b422dd57097": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
